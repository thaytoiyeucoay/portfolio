import { NextRequest, NextResponse } from 'next/server';
import { GoogleGenerativeAI } from '@google/generative-ai';

export const runtime = 'nodejs';

function extractTags(caption: string, limit = 6): string[] {
  const stop = new Set([
    'the','a','an','and','or','with','without','of','on','in','at','to','from','by','over','under','for','is','are','be','it','this','that','these','those','as','into','while','near','a','an','the'
  ]);
  return Array.from(
    new Set(
      caption
        .toLowerCase()
        .replace(/[^a-z0-9\s-]/g, ' ')
        .split(/\s+/)
        .filter(w => w && !stop.has(w) && w.length > 2)
    )
  ).slice(0, limit);
}

async function callHF(bytes: Uint8Array): Promise<string> {
  const apiKey = process.env.HF_API_KEY || process.env.HUGGINGFACE_API_KEY;
  if (!apiKey) throw new Error('Missing HF_API_KEY');
  const preferred = process.env.HF_IMAGE_CAPTION_MODEL;
  const models = [
    preferred?.trim(),
    'Salesforce/blip-image-captioning-base',
    'nlpconnect/vit-gpt2-image-captioning',
  ].filter(Boolean) as string[];
  let lastErr: any = null;
  for (const model of models) {
    try {
      // Attempt 1: raw bytes
      let res = await fetch(`https://api-inference.huggingface.co/models/${model}`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${apiKey}`,
          'Content-Type': 'application/octet-stream',
          'Accept': 'application/json',
          'X-Wait-For-Model': 'true',
        },
        body: bytes as any,
      });
      if (!res.ok && (res.status === 404 || res.status === 415)) {
        // Attempt 2: JSON base64
        const base64 = Buffer.from(bytes).toString('base64');
        res = await fetch(`https://api-inference.huggingface.co/models/${model}`, {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${apiKey}`,
            'Content-Type': 'application/json',
            'Accept': 'application/json',
            'X-Wait-For-Model': 'true',
          },
          body: JSON.stringify({ inputs: base64 }) as any,
        });
      }
      if (!res.ok) {
        const t = await res.text();
        if (res.status === 404 || res.status === 503) {
          lastErr = new Error(`HF error ${res.status} on ${model}: ${t}`);
          continue;
        }
        throw new Error(`HF error ${res.status} on ${model}: ${t}`);
      }
      const data: any = await res.json();
      // Normalize candidates
      const candidates = Array.isArray(data)
        ? data
            .map((d: any) => ({ text: d?.generated_text || d?.caption || '', score: d?.score }))
            .filter((d: any) => d.text)
        : [];
      if (!candidates.length) throw new Error(`No caption generated by ${model}`);
      // Attach to global for later use in response via closure? We'll return only best text here, and re-parse again in POST.
      return String(candidates[0].text);
    } catch (e) {
      lastErr = e;
      continue;
    }
  }
  throw lastErr || new Error('All caption models failed');
}

async function callGemini(bytes: Uint8Array): Promise<string> {
  const apiKey = process.env.GEMINI_API_KEY;
  if (!apiKey) throw new Error('Missing GEMINI_API_KEY');
  const genAI = new GoogleGenerativeAI(apiKey);
  const model = genAI.getGenerativeModel({ model: process.env.GEMINI_IMAGE_MODEL || 'gemini-1.5-flash' });
  const b64 = Buffer.from(bytes).toString('base64');
  const res = await model.generateContent([
    {
      inlineData: { data: b64, mimeType: 'image/jpeg' }, // mime sẽ auto, nhưng jpeg an toàn cho suy luận
    } as any,
    { text: 'Mô tả ngắn gọn bằng tiếng Việt cho bức ảnh trên (dưới 20 từ).' },
  ]);
  const text = res.response.text().trim();
  if (!text) throw new Error('Gemini returned empty text');
  return text;
}

export async function POST(req: NextRequest) {
  try {
    const ctype = req.headers.get('content-type') || '';
    let imageBytes: Uint8Array | null = null;

    if (ctype.includes('multipart/form-data')) {
      const form = await req.formData();
      const file = form.get('file') as unknown as File | null;
      if (file) {
        const arrBuf = await file.arrayBuffer();
        imageBytes = new Uint8Array(arrBuf);
      }
    } else {
      const body = await req.json().catch(() => ({}));
      const url = body?.imageUrl as string | undefined;
      if (url) {
        const r = await fetch(url);
        const buf = new Uint8Array(await r.arrayBuffer());
        imageBytes = buf;
      }
    }

    if (!imageBytes) {
      return NextResponse.json({ error: 'No image provided' }, { status: 400 });
    }

    let caption: string;
    try {
      caption = await callHF(imageBytes);
    } catch (e) {
      // Fallback to Gemini if available
      if (process.env.GEMINI_API_KEY) {
        caption = await callGemini(imageBytes);
      } else {
        throw e;
      }
    }
    // Attempt to get multiple candidates again by querying preferred model with JSON base64 to fetch list when possible
    const apiKey = process.env.HF_API_KEY || process.env.HUGGINGFACE_API_KEY;
    const modelName = process.env.HF_IMAGE_CAPTION_MODEL || 'Salesforce/blip-image-captioning-base';
    let candidates: Array<{ text: string; score?: number }> = [];
    if (apiKey) {
      try {
        const res = await fetch(`https://api-inference.huggingface.co/models/${modelName}`, {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${apiKey}`,
            'Content-Type': 'application/json',
            'Accept': 'application/json',
            'X-Wait-For-Model': 'true',
          },
          body: JSON.stringify({ inputs: Buffer.from(imageBytes).toString('base64') }) as any,
        });
        if (res.ok) {
          const data: any = await res.json();
          candidates = Array.isArray(data)
            ? data.map((d: any) => ({ text: d?.generated_text || d?.caption || '', score: d?.score })).filter((d: any) => d.text)
            : [];
        }
      } catch {}
    }

    // Fallback single candidate if list empty
    if (!candidates.length) candidates = [{ text: caption }];
    const primary = candidates[0]?.text || caption;
    const tags = extractTags(primary, 6);

    return NextResponse.json({ success: true, data: { caption: primary, tags, candidates } });
  } catch (e: any) {
    console.error('Caption API error:', e);
    return NextResponse.json({ error: e?.message || 'Internal error' }, { status: 500 });
  }
}
